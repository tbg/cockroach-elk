input {
    tcp {
        port => 5000
    }
    stdin { }
}

filter {
    json {
        source => "message"
    }
    # Hack to work around awkward JSON serialization: Each logged argument
    # contains a pre-serialized JSON string, but the JSON filter cannot iterate
    # over an array. Besides, go base64 encodes the JSON (because it's a byte
    # slice) which the json filter wouldn't be able to handle anways. The
    # solution for now is to just dive into Ruby and perform the right thing by
    # hand, which is probably slow but works like a charm. Cockroach should
    # probably just serialize this correctly right away when spitting out the
    # JSON log lines - currently we get JSON within JSON - but that requires a
    # gogoproto custom type and in turn implementations of a lot of things.
    # DISABLED since elasticsearch tries to index that field but its structure
    # is fluid, which drops most log messages which have something here.
     ruby {
        #code => "require 'json'; require 'base64'; (event['args'] or []).each{ |e| e['json'] = JSON.parse('['+(Base64.decode64((e['json'] or '')))+']').pop() }"
        # Just keep it as a string for now; better than nothing.
        code => "require 'base64'; (event['args'] or []).each{ |e| e['json'] = Base64.decode64((e['json'] or '')) }"
    }
    mutate {
        remove_field => ["message"]
    }
}

# Must use http protocol and specify the host to use Kibana4
output {
	elasticsearch { 
		protocol => "http"
		host => "elasticsearch"
	}
}
